{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link Prediction for Co-Authorship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.linalg import norm\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read True Edges from File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_edges(file_path):\n",
    "    train_file = open(file_path, \"r\")\n",
    "    train_text = re.split('\\n', train_file.read())\n",
    "    instances = [re.split('\\W+', instance) for instance in train_text]\n",
    "    true_edges = []\n",
    "    fake_edges = []\n",
    "    neighbour_dic = {}\n",
    "    i = 0\n",
    "    for j, instance in enumerate(instances):\n",
    "        if len(instance) > 1:\n",
    "            while(int(instance[0]) != i):\n",
    "                neighbour_dic[i] = set()\n",
    "                i += 1\n",
    "            neighbour_dic[int(instance[0])] = {i for i in instance[1:] if int(i)!=int(instance[0])}\n",
    "            i += 1\n",
    "        true_edges += [(int(instance[0]), int(instance[i])) for i in range(1, len(instance)) if int(instance[i])>int(instance[0])]\n",
    "        fake_edges += [(int(instance[0]), i) for i in range(len(instances)) if len(instance)>1 and str(i) not in instance[1:] and i > int(instance[0])]\n",
    "    return true_edges, fake_edges, neighbour_dic\n",
    "true_edges, fake_edges, neighbour_dic = get_true_edges('train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7910572\n"
     ]
    }
   ],
   "source": [
    "fake_edges = fake_edges\n",
    "print(len(fake_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Test Edges From File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_edges(file_path):\n",
    "    test_file = open(file_path, \"r\")\n",
    "    test_text = re.split('\\n', test_file.read())  # edges: 57889-4084=53805\n",
    "    instances = [re.split('\\W+', instance) for instance in test_text]\n",
    "    test_edges = [(int(instance[1]), int(instance[2])) for instance in instances[1:] if len(instance)==3]\n",
    "    return test_edges\n",
    "test_edges = get_test_edges('test-public.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Node Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_feature_from_json(file_path):\n",
    "    with open(file_path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        feature_data = {}\n",
    "        for author in data:\n",
    "            id = author['id']\n",
    "            feature_data[id] = {}\n",
    "            feature_data[id] = author\n",
    "            feature_data[id].pop('id')\n",
    "        return feature_data\n",
    "node_file = 'nodes.json'\n",
    "node_feature_dic = get_node_feature_from_json(node_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(true_edges):\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(true_edges)\n",
    "    return G\n",
    "G = generate_graph(true_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Negative Samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_slice = []\n",
    "shortest_distance_2 = 3000\n",
    "for edge in set(fake_edges):\n",
    "    a1,a2 = edge\n",
    "    d = 0\n",
    "    if G.has_node(a1) and G.has_node(a2):\n",
    "        if nx.has_path(G, source=a1, target=a2):\n",
    "            if G.has_edge(edge[0],edge[1]):\n",
    "                G.remove_edge(a1,a2)\n",
    "                try: d = nx.dijkstra_path_length(G, source=a1, target=a2)\n",
    "                except: d = 0\n",
    "                G.add_edge(a1,a2)\n",
    "            else:\n",
    "                d = nx.dijkstra_path_length(G, source=a1, target=a2)\n",
    "    if d==2: \n",
    "        fake_slice.append(edge)\n",
    "        shortest_distance_2 -= 1\n",
    "    elif len(fake_slice) + shortest_distance_2 <= len(true_edges):\n",
    "        fake_slice.append(edge)\n",
    "    else: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_Data(true_edges):\n",
    "    Data = np.zeros([len(true_edges), 2])\n",
    "    for i in range(len(true_edges)):\n",
    "        Data[i][0] = true_edges[i][0]\n",
    "        Data[i][1] = true_edges[i][1]\n",
    "    return Data\n",
    "def get_max_node_num(Data):\n",
    "    List_A = []\n",
    "    List_B = []\n",
    "    for row in range(Data.shape[0]):\n",
    "        List_A.append(Data[row][0])\n",
    "        List_B.append(Data[row][1])\n",
    "    List_A = list(set(List_A))\n",
    "    List_B = list(set(List_B))\n",
    "    max_node_num = int(max(max(List_A), max(List_B))) + 1\n",
    "    return max_node_num\n",
    "    \n",
    "def get_Adjacency(max_node_num, Data):\n",
    "    Adjacency = np.zeros([max_node_num, max_node_num])\n",
    "    for c in range(Data.shape[0]):\n",
    "        i = int(Data[c][0])\n",
    "        j = int(Data[c][1])\n",
    "        Adjacency[i,j] = 1\n",
    "        Adjacency[j,i] = 1\n",
    "    return Adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_edges = list(true_edges)\n",
    "Data = get_true_Data(true_edges)\n",
    "max_node_num = get_max_node_num(Data)\n",
    "Adjacency = get_Adjacency(max_node_num, Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data into Training data and Test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42436 10609\n"
     ]
    }
   ],
   "source": [
    "train_edges = true_edges+fake_slice\n",
    "labels = [1]*len(true_edges) + [0]*len(fake_slice)\n",
    "from sklearn.model_selection import train_test_split\n",
    "trn_edges, dev_edges, trn_y, dev_y = train_test_split(train_edges, labels, test_size=0.2, random_state=999999,\n",
    "                                                  stratify=labels)\n",
    "\n",
    "trn_true_edges = [i for i in trn_edges if G.has_edge(i[0],i[1])]\n",
    "print(len(trn_edges), len(dev_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_Data = get_true_Data(trn_true_edges)\n",
    "trn_Adjacency = get_Adjacency(max_node_num, trn_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topological Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Salton(Adjacency_Train):\n",
    "    similarity = np.dot(Adjacency_Train,Adjacency_Train)\n",
    "    deg_row = sum(Adjacency_Train)\n",
    "    deg_row.shape = (deg_row.shape[0],1)\n",
    "    deg_row_T = deg_row.T\n",
    "    tempdeg = np.dot(deg_row,deg_row_T)\n",
    "    temp = np.sqrt(tempdeg+1)\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    Matrix_similarity = np.nan_to_num(similarity / temp)\n",
    "    return Matrix_similarity\n",
    "def Jaccard(Adjacency_Train):    \n",
    "    Matrix_similarity = np.dot(Adjacency_Train,Adjacency_Train)\n",
    "    deg_row = sum(Adjacency_Train)\n",
    "    deg_row.shape = (deg_row.shape[0],1)\n",
    "    deg_row_T = deg_row.T\n",
    "    tempdeg = deg_row + deg_row_T\n",
    "    temp = tempdeg - Matrix_similarity\n",
    "    Matrix_similarity = Matrix_similarity / (temp+1)\n",
    "    return Matrix_similarity\n",
    "def Sorenson(Adjacency_Train):\n",
    "    Matrix_similarity = np.dot(Adjacency_Train,Adjacency_Train)\n",
    "    deg_row = sum(Adjacency_Train)\n",
    "    deg_row.shape = (deg_row.shape[0],1)\n",
    "    deg_row_T = deg_row.T\n",
    "    tempdeg = deg_row + deg_row_T + 1\n",
    "    Matrix_similarity = (2 * Matrix_similarity) / tempdeg\n",
    "    return Matrix_similarity\n",
    "def HDI(Adjacency_Train):\n",
    "    Matrix_similarity = np.dot(Adjacency_Train,Adjacency_Train)\n",
    "    deg_row = sum(Adjacency_Train)\n",
    "    deg_row.shape = (deg_row.shape[0],1)\n",
    "    deg_row_T = deg_row.T\n",
    "    tempdeg = np.maximum(deg_row,deg_row_T)\n",
    "    Matrix_similarity = Matrix_similarity / (tempdeg+1)\n",
    "    return Matrix_similarity\n",
    "\n",
    "def AA(Adjacency_Train):\n",
    "    logTrain = np.log((1+sum(Adjacency_Train)))\n",
    "    logTrain = np.nan_to_num(logTrain)\n",
    "    logTrain.shape = (logTrain.shape[0],1)\n",
    "    Adjacency_Train_Log = Adjacency_Train / logTrain\n",
    "    Adjacency_Train_Log = np.nan_to_num(Adjacency_Train_Log)\n",
    "    Matrix_similarity = np.dot(Adjacency_Train,Adjacency_Train_Log)\n",
    "    return Matrix_similarity\n",
    "def Katz(Adjacency_Train):\n",
    "    Parameter = 0.01\n",
    "    Matrix_EYE = np.eye(Adjacency_Train.shape[0])\n",
    "    Temp = Matrix_EYE - Adjacency_Train * Parameter\n",
    "    Matrix_similarity = np.linalg.inv(Temp)\n",
    "    Matrix_similarity = Matrix_similarity - Matrix_EYE\n",
    "    return Matrix_similarity\n",
    "def Cos(Adjacency_Train):     \n",
    "    Matrix_D = np.diag(sum(Adjacency_Train))\n",
    "    Matrix_Laplacian = Matrix_D - Adjacency_Train\n",
    "    INV_Matrix_Laplacian  = np.linalg.pinv(Matrix_Laplacian)\n",
    "    Array_Diag = np.diag(INV_Matrix_Laplacian)\n",
    "    Matrix_ONE = np.ones([Adjacency_Train.shape[0],Adjacency_Train.shape[0]])\n",
    "    Matrix_Diag = Array_Diag * Matrix_ONE\n",
    "    Matrix_similarity = INV_Matrix_Laplacian/((Matrix_Diag * Matrix_Diag.T) ** 0.5)\n",
    "    Matrix_similarity = np.nan_to_num(Matrix_similarity)\n",
    "    return Matrix_similarity\n",
    "def get_SIM(edges, matrix):\n",
    "    array = []\n",
    "    for edge in edges:\n",
    "        array.append(matrix[edge[0]][edge[1]])\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix_CN = np.dot(Adjacency,Adjacency)\n",
    "Matrix_Salton = Salton(Adjacency)\n",
    "Matrix_Jaccard = Jaccard(Adjacency)\n",
    "Matrix_Sorenson = Sorenson(Adjacency)\n",
    "Matrix_HDI = HDI(Adjacency)\n",
    "Matrix_AA = AA(Adjacency)\n",
    "Matrix_Katz = Katz(trn_Adjacency)\n",
    "Matrix_Cos = Cos(trn_Adjacency)\n",
    "# Matrix_Katz = Katz(Adjacency)\n",
    "# Matrix_Cos = Cos(Adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_cn, dev_cn, test_cn = get_SIM(trn_edges, Matrix_CN), get_SIM(dev_edges, Matrix_CN), get_SIM(test_edges, Matrix_CN)\n",
    "trn_sal, dev_sal, test_sal = get_SIM(trn_edges, Matrix_Salton), get_SIM(dev_edges, Matrix_Salton), get_SIM(test_edges, Matrix_Salton)\n",
    "trn_sor, dev_sor, test_sor = get_SIM(trn_edges, Matrix_Sorenson), get_SIM(dev_edges, Matrix_Sorenson), get_SIM(test_edges, Matrix_Sorenson)\n",
    "trn_jc, dev_jc, test_jc = get_SIM(trn_edges, Matrix_Jaccard), get_SIM(dev_edges, Matrix_Jaccard), get_SIM(test_edges, Matrix_Jaccard)\n",
    "trn_hdi, dev_hdi, test_hdi = get_SIM(trn_edges, Matrix_HDI), get_SIM(dev_edges, Matrix_HDI), get_SIM(test_edges, Matrix_HDI)\n",
    "\n",
    "trn_aa, dev_aa, test_aa = get_SIM(trn_edges, Matrix_AA), get_SIM(dev_edges, Matrix_AA), get_SIM(test_edges, Matrix_AA)\n",
    "trn_kz, dev_kz, test_kz = get_SIM(trn_edges, Matrix_Katz), get_SIM(dev_edges, Matrix_Katz), get_SIM(test_edges, Matrix_Katz)\n",
    "trn_cos, dev_cos, test_cos = get_SIM(trn_edges, Matrix_Cos), get_SIM(dev_edges, Matrix_Cos), get_SIM(test_edges, Matrix_Cos)\n",
    "trn_kw, dev_kw, test_kw = get_KW(trn_edges), get_KW(dev_edges), get_KW(test_edges)\n",
    "trn_vn, dev_vn, test_vn=  get_VN(trn_edges), get_VN(dev_edges), get_VN(test_edges)\n",
    "trn_td, dev_td,test_td = get_TD(trn_edges),get_TD(dev_edges),get_TD(test_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf():\n",
    "    kw_features = []\n",
    "    vn_features = []\n",
    "    for node_attri in node_feature_dic.values():\n",
    "        kw_feature = ''\n",
    "        vn_feature = ''\n",
    "        for attri in node_attri.keys():\n",
    "            if 'keyword_' in attri:\n",
    "                kw_feature += (attri+' ')\n",
    "            elif 'venue_' in attri:\n",
    "                vn_feature += (attri+' ')\n",
    "        kw_features.append(kw_feature)\n",
    "        vn_features.append(vn_feature)\n",
    "    tv = TfidfVectorizer()\n",
    "    feature_vectors = tv.fit_transform(kw_features).toarray()\n",
    "    venue_vectors = tv.fit_transform(vn_features).toarray()\n",
    "    return feature_vectors, venue_vectors\n",
    "\n",
    "def get_tfidf_similarity(vectors,edge):\n",
    "    node1,node2 = int(edge[0]),int(edge[1])\n",
    "    if (norm(vectors[node1]) * norm(vectors[node2]))==0:\n",
    "        return 0\n",
    "    similarity = np.dot(vectors[node1], vectors[node2]) / (norm(vectors[node1]) * norm(vectors[node2]))\n",
    "    return similarity\n",
    "feature_vectors, venue_vectors = get_tfidf()\n",
    "def get_ckw(edges):\n",
    "    array = []\n",
    "    for edge in edges:\n",
    "        a1,a2 = edge\n",
    "        keyword_a1 = {kw for kw in node_feature_dic[a1] if 'keyword_' in kw}\n",
    "        keyword_a2 = {kw for kw in node_feature_dic[a2] if 'keyword_' in kw}\n",
    "        intersect = keyword_a1.intersection(keyword_a2)\n",
    "        array.append(len(intersect)/max(1,min(len(keyword_a1), len(keyword_a2))))\n",
    "    return array\n",
    "def get_cvn(edges):\n",
    "    array = []\n",
    "    for edge in edges:\n",
    "        a1,a2 = edge\n",
    "        keyword_a1 = {kw for kw in node_feature_dic[a1] if 'venue_' in kw}\n",
    "        keyword_a2 = {kw for kw in node_feature_dic[a2] if 'venue_' in kw}\n",
    "        intersect = keyword_a1.intersection(keyword_a2)\n",
    "        array.append(len(intersect)/max(1,min(len(keyword_a1), len(keyword_a2))))\n",
    "    return array\n",
    "def get_jckw(edges):\n",
    "    array = []\n",
    "    for edge in edges:\n",
    "        a1,a2 = edge\n",
    "        set1 = {kw for kw in node_feature_dic[a1] if 'keyword_' in kw}\n",
    "        set2 = {kw for kw in node_feature_dic[a2] if 'keyword_' in kw}\n",
    "        intersect = set1.intersection(set2)\n",
    "        union = set1.union(set2)\n",
    "        array.append(len(intersect)/max(1,len(union)))\n",
    "    return array\n",
    "def get_jcvn(edges):\n",
    "    array = []\n",
    "    for edge in edges:\n",
    "        a1,a2 = edge\n",
    "        set1 = {kw for kw in node_feature_dic[a1] if 'venue_' in kw}\n",
    "        set2 = {kw for kw in node_feature_dic[a2] if 'venue_' in kw}\n",
    "        intersect = set1.intersection(set2)\n",
    "        union = set1.union(set2)\n",
    "        array.append(len(intersect)/max(1,len(union)))\n",
    "\n",
    "    return array\n",
    "def get_KW(edges):\n",
    "    array = []\n",
    "    for edge in edges:\n",
    "        kw = get_tfidf_similarity(feature_vectors, edge)\n",
    "        array.append(kw)\n",
    "    return array\n",
    "\n",
    "def get_VN(edges):\n",
    "    array = []\n",
    "    for edge in edges:\n",
    "        vn = get_tfidf_similarity(venue_vectors, edge)\n",
    "        array.append(vn)\n",
    "    return array\n",
    "def get_TD(edges):\n",
    "    array = []\n",
    "    for edge in edges:\n",
    "        diff = abs(node_feature_dic[edge[0]]['last']-node_feature_dic[edge[1]]['last'])\n",
    "        if diff == 0: array.append(1)\n",
    "        else:array.append(1/(diff+1))\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ckw, dev_ckw, test_ckw = get_ckw(trn_edges), get_ckw(dev_edges), get_ckw(test_edges)\n",
    "trn_cvn, dev_cvn, test_cvn = get_cvn(trn_edges), get_cvn(dev_edges), get_cvn(test_edges)\n",
    "trn_jckw, dev_jckw, test_jckw = get_jckw(trn_edges), get_jckw(dev_edges), get_jckw(test_edges)\n",
    "trn_jcvn, dev_jcvn, test_jcvn = get_jcvn(trn_edges), get_jcvn(dev_edges), get_jcvn(test_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Link Reliability based on Community**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BC(G):\n",
    "    edge_bt = nx.edge_betweenness_centrality(G,normalized=True)\n",
    "    bc = [(edge, bc) for edge, bc in edge_bt.items()]\n",
    "    bc = sorted(bc, key=lambda x:x[1], reverse=True)\n",
    "    return bc\n",
    "def reliability(components):\n",
    "    size = len(components)\n",
    "    Block = np.zeros([size, size])\n",
    "    HP = 0\n",
    "    for i, c1 in enumerate(components):\n",
    "        for j, c2 in enumerate(components):\n",
    "            if i<=j:\n",
    "                L = A[list(c1)]\n",
    "                L = L[:,list(c2)]\n",
    "                l = np.sum(L) if i!=j else np.sum(L)/2\n",
    "                r = len(c1)*len(c2) if i!=j else len(c1)*(len(c1)-1)/2\n",
    "                Block[i][j] = (l+1)/(r+2)\n",
    "                Block[j][i] = (l+1)/(r+2)\n",
    "                HP += (math.log(r+1)+ math.log(comb(max(int(r),1),max(int(l),1))))\n",
    "    return Block, HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1432,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2 = G.copy() \n",
    "Blocks = []\n",
    "HPs = []\n",
    "Nodes = list(G.nodes)\n",
    "for n in Nodes:\n",
    "    G.nodes[n]['community'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(nx.number_of_edges(G2)>0):\n",
    "    \n",
    "    betweenness = BC(G2)\n",
    "    a1, a2 = betweenness[0][0]\n",
    "    if betweenness[0][1]==0: break\n",
    "    initial_components = sorted([i for i in nx.connected_components(G2)], key=lambda x:len(x), reverse=True)\n",
    "    initial_subgraph = [G2.subgraph(c).copy() for c in nx.connected_components(G2)]\n",
    "    \n",
    "    G2.remove_edge(a1, a2)\n",
    "    \n",
    "    components = sorted([i for i in nx.connected_components(G2)], key=lambda x:len(x), reverse=True)\n",
    "    subgraph = [G2.subgraph(c) for c in nx.connected_components(G2)]\n",
    "    \n",
    "    length = [nx.number_of_edges(c) for c in subgraph if nx.number_of_edges(c)>10]\n",
    "    node_num = [len(i) for i in nx.connected_components(G2) if len(i)>10]\n",
    "    print(nx.number_of_edges(G2),' edges left, ',length,node_num )\n",
    "        \n",
    "        \n",
    "    if len(components)>len(initial_components):\n",
    "        Block, HP = reliability(components)\n",
    "        Blocks.append(Block)\n",
    "        HPs.append(HP)\n",
    "        for i,c in enumerate(components):\n",
    "            for n in c: G.nodes[n]['community'].append(i)\n",
    "        print(len(length),' communities now')\n",
    "        print('************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_reliability(edges):\n",
    "    array = []\n",
    "    for edge in edges:\n",
    "        a1,a2 = edge\n",
    "        R = 0\n",
    "        Z = 0\n",
    "        if G.has_node(a1) and G.has_node(a2):\n",
    "            a1_community = G.nodes[a1]['community']\n",
    "            a2_community = G.nodes[a2]['community']\n",
    "\n",
    "            for i in range(len(Blocks)):\n",
    "                a1_c = a1_community[i]\n",
    "                a2_c = a2_community[i] \n",
    "                r = Blocks[i][int(a1_c)][int(a2_c)]\n",
    "                p = (math.e)**(-HPs[i])\n",
    "                R += r\n",
    "                Z += p\n",
    "            R = R/(Z+1)\n",
    "        array.append(R)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1335,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_rl, dev_rl, test_rl = link_reliability(trn_edges), link_reliability(dev_edges), link_reliability(test_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(select, feature_sequence):\n",
    "    X = []\n",
    "    Dic = []\n",
    "    if select['KW']: X.append(feature_sequence[0])\n",
    "    if select['VN']: X.append(feature_sequence[1])\n",
    "    if select['TD']: X.append(feature_sequence[2])\n",
    "\n",
    "    if select['CN']: X.append(feature_sequence[3])\n",
    "    if select['SAL']: X.append(feature_sequence[4])\n",
    "    if select['SOR']: X.append(feature_sequence[5])\n",
    "    if select['JC']: X.append(feature_sequence[6])  \n",
    "    if select['HDI']: X.append(feature_sequence[7])\n",
    "\n",
    "    if select['AA']: X.append(feature_sequence[8])\n",
    "    if select['KZ']: X.append(feature_sequence[9])\n",
    "    if select['COS']: X.append(feature_sequence[10])\n",
    "    if select['CKW']: X.append(feature_sequence[11])\n",
    "    if select['CVN']: X.append(feature_sequence[12])\n",
    "    if select['JCKW']: X.append(feature_sequence[13])\n",
    "    if select['JCVN']: X.append(feature_sequence[14])\n",
    "    if select['RL']: X.append(feature_sequence[15])\n",
    "    for i in range(len(X[0])):\n",
    "        diff = {}\n",
    "        if select['KW']: diff['kw'] = feature_sequence[0][i]\n",
    "        if select['VN']: diff['vn'] = feature_sequence[1][i]\n",
    "        if select['TD']: diff['td'] = feature_sequence[2][i]\n",
    "\n",
    "        if select['CN']: diff['cn'] = feature_sequence[3][i]\n",
    "        if select['SAL']: diff['sal'] = feature_sequence[4][i]\n",
    "        if select['SOR']: diff['sor'] = feature_sequence[5][i]\n",
    "        if select['JC']: diff['jc'] = feature_sequence[6][i] \n",
    "        if select['HDI']: diff['hdi'] = feature_sequence[7][i]\n",
    "\n",
    "        if select['AA']: diff['aa'] = feature_sequence[8][i]\n",
    "        if select['KZ']: diff['kz'] = feature_sequence[9][i]\n",
    "        if select['COS']: diff['cos'] = feature_sequence[10][i]\n",
    "        if select['CKW']: diff['ckw'] = feature_sequence[11][i]\n",
    "        if select['CVN']: diff['cvn'] = feature_sequence[12][i]\n",
    "        if select['JCKW']: diff['jckw'] = feature_sequence[13][i]\n",
    "        if select['JCVN']: diff['jcvn'] = feature_sequence[14][i]\n",
    "        if select['RL']: diff['rl'] = feature_sequence[15][i]\n",
    "        Dic.append(diff)\n",
    "    return np.array(X).T, Dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_select = {'KW':True,'VN':True,'TD':True,\n",
    "                  'CN':True,'SAL':True, 'SOR':True,'JC':True, 'HDI':True,\n",
    "                  'AA':True,'KZ':True, 'COS':True,\n",
    "                 'CKW':True, 'CVN':True, 'JCKW':True, 'JCVN':True,'RL':False}\n",
    "trn_x, trn_x_dic = get_features(feature_select,[trn_kw, trn_vn, trn_td, trn_cn, trn_sal, trn_sor, trn_jc, trn_hdi,\n",
    "                                                trn_aa, trn_kz, trn_cos, trn_ckw, trn_cvn, trn_jckw, trn_jcvn, []])\n",
    "\n",
    "dev_x, dev_x_dic = get_features(feature_select,[dev_kw, dev_vn, dev_td, dev_cn, dev_sal, dev_sor, dev_jc, dev_hdi,\n",
    "                                                dev_aa, dev_kz, dev_cos, dev_ckw, dev_cvn, dev_jckw, dev_jcvn, []])\n",
    "\n",
    "test_x, test_x_dic = get_features(feature_select,[test_kw, test_vn, test_td, test_cn, test_sal, test_sor, test_jc, test_hdi,\n",
    "                                                test_aa, test_kz, test_cos, test_ckw, test_cvn, test_jckw, test_jcvn,[]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot(clfs, test_auc_list, c_test):\n",
    "    p = plt.bar([num + 0.25 for num in range(len(clfs))], test_auc_list, 0.5)\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title('AUC classifying edge, by classifer')\n",
    "    plt.ylim([0.6, 1])\n",
    "    plt.xticks([num + 0.5 for num in range(len(clfs))], (c_test))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(trn_X,dev_X,trn_y,dev_y):\n",
    "    parameters = [0.001,0.01,0.1,1,10,10**2,10**3,10**4]\n",
    "\n",
    "    clfs = [LogisticRegression(penalty='l2',C=c,class_weight='balanced',solver='liblinear') for c in parameters]\n",
    "    auc_list = []\n",
    "    best_auc = 0\n",
    "    best_clf = None\n",
    "    best_predictions = None\n",
    "    best_prob= None\n",
    "#     for clf in clfs:\n",
    "        clf.fit(trn_X, trn_y)\n",
    "        predictions = clf.predict(dev_X)\n",
    "        predict_prob = clf.predict_proba(dev_X)\n",
    "\n",
    "        auc = round(roc_auc_score(dev_y, predictions), 4)\n",
    "        if auc > best_auc: \n",
    "            best_clf = clf\n",
    "            best_auc = auc\n",
    "            best_predictions = predictions\n",
    "            best_prob = predict_prob\n",
    "        auc_list.append(auc)\n",
    "    print('auc:', auc_list)\n",
    "    print('best auc:',best_auc)\n",
    "    print('dev_length',len(dev_y))\n",
    "    print('error:',len([1 for i,p in enumerate(best_predictions) if p!= dev_y[i]]))\n",
    "    print([i for i in best_clf.coef_])\n",
    "    plot(clfs, auc_list, parameters)\n",
    "\n",
    "    return best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42436, 15) 42436\n",
      "(10609, 15) 10609\n",
      "auc: [0.9081, 0.9256, 0.9324, 0.9254, 0.9177, 0.8587, 0.8161, 0.6899]\n",
      "best auc: 0.9324\n",
      "dev_length 10609\n",
      "error: 724\n",
      "[array([ 6.52391207,  1.13586748, -1.17523139, 10.88672589,  1.19186443,\n",
      "        0.86732033,  0.7373112 ,  0.71329217,  0.20143907,  1.88837192,\n",
      "        1.57353786,  2.37009708,  1.5479989 ,  0.04155648,  3.03513803])]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer = DictVectorizer()\n",
    "trn_X = vectorizer.fit_transform(trn_x_dic)\n",
    "dev_X = vectorizer.transform(dev_x_dic)\n",
    "test_X = vectorizer.transform(test_x_dic)\n",
    "print(trn_X.shape, len(trn_y))\n",
    "print(dev_X.shape,len(dev_y))\n",
    "best_clf = train(trn_X, dev_X, trn_y, dev_y)\n",
    "predictions = best_clf.predict_proba(test_X)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42436 samples\n",
      "Epoch 1/10\n",
      "42436/42436 [==============================] - ETA: 37:25 - loss: 0.6610 - auc_6: 0.5000 - accuracy: 0.406 - ETA: 1:23 - loss: 0.6226 - auc_6: 0.8311 - accuracy: 0.627 - ETA: 41s - loss: 0.5967 - auc_6: 0.8749 - accuracy: 0.6943 - ETA: 26s - loss: 0.5721 - auc_6: 0.8909 - accuracy: 0.713 - ETA: 20s - loss: 0.5501 - auc_6: 0.8891 - accuracy: 0.715 - ETA: 15s - loss: 0.5283 - auc_6: 0.8856 - accuracy: 0.713 - ETA: 12s - loss: 0.5082 - auc_6: 0.8869 - accuracy: 0.713 - ETA: 10s - loss: 0.4900 - auc_6: 0.8938 - accuracy: 0.721 - ETA: 9s - loss: 0.4722 - auc_6: 0.8978 - accuracy: 0.731 - ETA: 8s - loss: 0.4560 - auc_6: 0.9021 - accuracy: 0.74 - ETA: 7s - loss: 0.4418 - auc_6: 0.9032 - accuracy: 0.75 - ETA: 6s - loss: 0.4282 - auc_6: 0.9051 - accuracy: 0.76 - ETA: 6s - loss: 0.4161 - auc_6: 0.9068 - accuracy: 0.77 - ETA: 5s - loss: 0.4033 - auc_6: 0.9074 - accuracy: 0.77 - ETA: 5s - loss: 0.3929 - auc_6: 0.9083 - accuracy: 0.78 - ETA: 4s - loss: 0.3828 - auc_6: 0.9092 - accuracy: 0.78 - ETA: 4s - loss: 0.3733 - auc_6: 0.9107 - accuracy: 0.79 - ETA: 4s - loss: 0.3648 - auc_6: 0.9120 - accuracy: 0.79 - ETA: 3s - loss: 0.3568 - auc_6: 0.9140 - accuracy: 0.80 - ETA: 3s - loss: 0.3501 - auc_6: 0.9153 - accuracy: 0.80 - ETA: 3s - loss: 0.3433 - auc_6: 0.9164 - accuracy: 0.81 - ETA: 3s - loss: 0.3371 - auc_6: 0.9175 - accuracy: 0.81 - ETA: 3s - loss: 0.3312 - auc_6: 0.9186 - accuracy: 0.81 - ETA: 3s - loss: 0.3259 - auc_6: 0.9198 - accuracy: 0.82 - ETA: 2s - loss: 0.3211 - auc_6: 0.9207 - accuracy: 0.82 - ETA: 2s - loss: 0.3168 - auc_6: 0.9210 - accuracy: 0.82 - ETA: 2s - loss: 0.3124 - auc_6: 0.9223 - accuracy: 0.82 - ETA: 2s - loss: 0.3083 - auc_6: 0.9230 - accuracy: 0.83 - ETA: 2s - loss: 0.3042 - auc_6: 0.9237 - accuracy: 0.83 - ETA: 2s - loss: 0.3002 - auc_6: 0.9236 - accuracy: 0.83 - ETA: 2s - loss: 0.2962 - auc_6: 0.9250 - accuracy: 0.83 - ETA: 2s - loss: 0.2922 - auc_6: 0.9253 - accuracy: 0.83 - ETA: 1s - loss: 0.2885 - auc_6: 0.9258 - accuracy: 0.84 - ETA: 1s - loss: 0.2847 - auc_6: 0.9260 - accuracy: 0.84 - ETA: 1s - loss: 0.2808 - auc_6: 0.9270 - accuracy: 0.84 - ETA: 1s - loss: 0.2773 - auc_6: 0.9274 - accuracy: 0.84 - ETA: 1s - loss: 0.2739 - auc_6: 0.9283 - accuracy: 0.84 - ETA: 1s - loss: 0.2703 - auc_6: 0.9293 - accuracy: 0.84 - ETA: 1s - loss: 0.2673 - auc_6: 0.9300 - accuracy: 0.85 - ETA: 1s - loss: 0.2642 - auc_6: 0.9306 - accuracy: 0.85 - ETA: 1s - loss: 0.2610 - auc_6: 0.9314 - accuracy: 0.85 - ETA: 0s - loss: 0.2582 - auc_6: 0.9317 - accuracy: 0.85 - ETA: 0s - loss: 0.2556 - auc_6: 0.9320 - accuracy: 0.85 - ETA: 0s - loss: 0.2529 - auc_6: 0.9329 - accuracy: 0.85 - ETA: 0s - loss: 0.2505 - auc_6: 0.9334 - accuracy: 0.85 - ETA: 0s - loss: 0.2476 - auc_6: 0.9341 - accuracy: 0.85 - ETA: 0s - loss: 0.2448 - auc_6: 0.9352 - accuracy: 0.86 - ETA: 0s - loss: 0.2422 - auc_6: 0.9356 - accuracy: 0.86 - ETA: 0s - loss: 0.2395 - auc_6: 0.9365 - accuracy: 0.86 - ETA: 0s - loss: 0.2369 - auc_6: 0.9370 - accuracy: 0.86 - ETA: 0s - loss: 0.2347 - auc_6: 0.9375 - accuracy: 0.86 - ETA: 0s - loss: 0.2325 - auc_6: 0.9380 - accuracy: 0.86 - 4s 101us/sample - loss: 0.2324 - auc_6: 0.9381 - accuracy: 0.8658\n",
      "Epoch 2/10\n",
      "42436/42436 [==============================] - ETA: 9s - loss: 0.1060 - auc_6: 0.9922 - accuracy: 0.93 - ETA: 3s - loss: 0.1237 - auc_6: 0.9702 - accuracy: 0.91 - ETA: 3s - loss: 0.1283 - auc_6: 0.9677 - accuracy: 0.90 - ETA: 2s - loss: 0.1240 - auc_6: 0.9726 - accuracy: 0.91 - ETA: 2s - loss: 0.1253 - auc_6: 0.9720 - accuracy: 0.90 - ETA: 2s - loss: 0.1243 - auc_6: 0.9723 - accuracy: 0.91 - ETA: 2s - loss: 0.1239 - auc_6: 0.9728 - accuracy: 0.90 - ETA: 2s - loss: 0.1225 - auc_6: 0.9740 - accuracy: 0.91 - ETA: 2s - loss: 0.1216 - auc_6: 0.9751 - accuracy: 0.91 - ETA: 2s - loss: 0.1229 - auc_6: 0.9740 - accuracy: 0.90 - ETA: 2s - loss: 0.1236 - auc_6: 0.9728 - accuracy: 0.90 - ETA: 2s - loss: 0.1230 - auc_6: 0.9731 - accuracy: 0.91 - ETA: 2s - loss: 0.1224 - auc_6: 0.9735 - accuracy: 0.91 - ETA: 2s - loss: 0.1220 - auc_6: 0.9736 - accuracy: 0.91 - ETA: 2s - loss: 0.1224 - auc_6: 0.9732 - accuracy: 0.91 - ETA: 2s - loss: 0.1219 - auc_6: 0.9737 - accuracy: 0.91 - ETA: 1s - loss: 0.1225 - auc_6: 0.9732 - accuracy: 0.91 - ETA: 1s - loss: 0.1225 - auc_6: 0.9729 - accuracy: 0.91 - ETA: 1s - loss: 0.1228 - auc_6: 0.9726 - accuracy: 0.90 - ETA: 1s - loss: 0.1225 - auc_6: 0.9726 - accuracy: 0.91 - ETA: 1s - loss: 0.1226 - auc_6: 0.9724 - accuracy: 0.90 - ETA: 1s - loss: 0.1222 - auc_6: 0.9728 - accuracy: 0.90 - ETA: 1s - loss: 0.1216 - auc_6: 0.9732 - accuracy: 0.91 - ETA: 1s - loss: 0.1208 - auc_6: 0.9737 - accuracy: 0.91 - ETA: 1s - loss: 0.1207 - auc_6: 0.9735 - accuracy: 0.91 - ETA: 1s - loss: 0.1206 - auc_6: 0.9734 - accuracy: 0.91 - ETA: 1s - loss: 0.1202 - auc_6: 0.9738 - accuracy: 0.91 - ETA: 1s - loss: 0.1200 - auc_6: 0.9738 - accuracy: 0.91 - ETA: 1s - loss: 0.1202 - auc_6: 0.9736 - accuracy: 0.91 - ETA: 1s - loss: 0.1199 - auc_6: 0.9738 - accuracy: 0.91 - ETA: 1s - loss: 0.1200 - auc_6: 0.9737 - accuracy: 0.91 - ETA: 1s - loss: 0.1198 - auc_6: 0.9738 - accuracy: 0.91 - ETA: 1s - loss: 0.1195 - auc_6: 0.9740 - accuracy: 0.91 - ETA: 1s - loss: 0.1195 - auc_6: 0.9740 - accuracy: 0.91 - ETA: 1s - loss: 0.1195 - auc_6: 0.9740 - accuracy: 0.91 - ETA: 0s - loss: 0.1194 - auc_6: 0.9739 - accuracy: 0.91 - ETA: 0s - loss: 0.1194 - auc_6: 0.9738 - accuracy: 0.91 - ETA: 0s - loss: 0.1194 - auc_6: 0.9738 - accuracy: 0.91 - ETA: 0s - loss: 0.1193 - auc_6: 0.9738 - accuracy: 0.91 - ETA: 0s - loss: 0.1191 - auc_6: 0.9739 - accuracy: 0.91 - ETA: 0s - loss: 0.1193 - auc_6: 0.9738 - accuracy: 0.91 - ETA: 0s - loss: 0.1194 - auc_6: 0.9737 - accuracy: 0.91 - ETA: 0s - loss: 0.1190 - auc_6: 0.9738 - accuracy: 0.91 - ETA: 0s - loss: 0.1188 - auc_6: 0.9740 - accuracy: 0.91 - ETA: 0s - loss: 0.1188 - auc_6: 0.9739 - accuracy: 0.91 - ETA: 0s - loss: 0.1189 - auc_6: 0.9738 - accuracy: 0.91 - ETA: 0s - loss: 0.1189 - auc_6: 0.9736 - accuracy: 0.91 - ETA: 0s - loss: 0.1188 - auc_6: 0.9737 - accuracy: 0.91 - ETA: 0s - loss: 0.1187 - auc_6: 0.9736 - accuracy: 0.91 - ETA: 0s - loss: 0.1186 - auc_6: 0.9737 - accuracy: 0.91 - ETA: 0s - loss: 0.1187 - auc_6: 0.9736 - accuracy: 0.91 - ETA: 0s - loss: 0.1185 - auc_6: 0.9737 - accuracy: 0.91 - ETA: 0s - loss: 0.1184 - auc_6: 0.9737 - accuracy: 0.91 - ETA: 0s - loss: 0.1181 - auc_6: 0.9739 - accuracy: 0.91 - 3s 64us/sample - loss: 0.1181 - auc_6: 0.9739 - accuracy: 0.9145\n",
      "Epoch 3/10\n",
      "42436/42436 [==============================] - ETA: 10s - loss: 0.0976 - auc_6: 1.0000 - accuracy: 0.937 - ETA: 2s - loss: 0.1188 - auc_6: 0.9733 - accuracy: 0.902 - ETA: 2s - loss: 0.1156 - auc_6: 0.9742 - accuracy: 0.91 - ETA: 2s - loss: 0.1153 - auc_6: 0.9745 - accuracy: 0.91 - ETA: 2s - loss: 0.1128 - auc_6: 0.9757 - accuracy: 0.91 - ETA: 2s - loss: 0.1108 - auc_6: 0.9769 - accuracy: 0.92 - ETA: 2s - loss: 0.1108 - auc_6: 0.9769 - accuracy: 0.92 - ETA: 2s - loss: 0.1115 - auc_6: 0.9762 - accuracy: 0.91 - ETA: 2s - loss: 0.1115 - auc_6: 0.9765 - accuracy: 0.91 - ETA: 2s - loss: 0.1119 - auc_6: 0.9757 - accuracy: 0.92 - ETA: 2s - loss: 0.1124 - auc_6: 0.9756 - accuracy: 0.91 - ETA: 1s - loss: 0.1129 - auc_6: 0.9754 - accuracy: 0.91 - ETA: 1s - loss: 0.1134 - auc_6: 0.9750 - accuracy: 0.91 - ETA: 1s - loss: 0.1132 - auc_6: 0.9753 - accuracy: 0.91 - ETA: 1s - loss: 0.1128 - auc_6: 0.9755 - accuracy: 0.91 - ETA: 1s - loss: 0.1128 - auc_6: 0.9754 - accuracy: 0.91 - ETA: 1s - loss: 0.1127 - auc_6: 0.9755 - accuracy: 0.91 - ETA: 1s - loss: 0.1125 - auc_6: 0.9757 - accuracy: 0.91 - ETA: 1s - loss: 0.1123 - auc_6: 0.9758 - accuracy: 0.91 - ETA: 1s - loss: 0.1122 - auc_6: 0.9760 - accuracy: 0.91 - ETA: 1s - loss: 0.1122 - auc_6: 0.9759 - accuracy: 0.91 - ETA: 1s - loss: 0.1123 - auc_6: 0.9759 - accuracy: 0.91 - ETA: 1s - loss: 0.1121 - auc_6: 0.9759 - accuracy: 0.91 - ETA: 1s - loss: 0.1122 - auc_6: 0.9759 - accuracy: 0.91 - ETA: 1s - loss: 0.1119 - auc_6: 0.9761 - accuracy: 0.91 - ETA: 1s - loss: 0.1120 - auc_6: 0.9760 - accuracy: 0.91 - ETA: 1s - loss: 0.1120 - auc_6: 0.9761 - accuracy: 0.91 - ETA: 1s - loss: 0.1117 - auc_6: 0.9763 - accuracy: 0.91 - ETA: 1s - loss: 0.1116 - auc_6: 0.9763 - accuracy: 0.92 - ETA: 1s - loss: 0.1117 - auc_6: 0.9761 - accuracy: 0.92 - ETA: 1s - loss: 0.1121 - auc_6: 0.9757 - accuracy: 0.91 - ETA: 0s - loss: 0.1118 - auc_6: 0.9758 - accuracy: 0.92 - ETA: 0s - loss: 0.1117 - auc_6: 0.9759 - accuracy: 0.91 - ETA: 0s - loss: 0.1117 - auc_6: 0.9759 - accuracy: 0.92 - ETA: 0s - loss: 0.1118 - auc_6: 0.9757 - accuracy: 0.91 - ETA: 0s - loss: 0.1118 - auc_6: 0.9758 - accuracy: 0.92 - ETA: 0s - loss: 0.1117 - auc_6: 0.9757 - accuracy: 0.92 - ETA: 0s - loss: 0.1118 - auc_6: 0.9756 - accuracy: 0.92 - ETA: 0s - loss: 0.1120 - auc_6: 0.9754 - accuracy: 0.91 - ETA: 0s - loss: 0.1119 - auc_6: 0.9754 - accuracy: 0.91 - ETA: 0s - loss: 0.1119 - auc_6: 0.9753 - accuracy: 0.91 - ETA: 0s - loss: 0.1119 - auc_6: 0.9753 - accuracy: 0.91 - ETA: 0s - loss: 0.1118 - auc_6: 0.9754 - accuracy: 0.91 - ETA: 0s - loss: 0.1119 - auc_6: 0.9753 - accuracy: 0.91 - ETA: 0s - loss: 0.1118 - auc_6: 0.9753 - accuracy: 0.91 - ETA: 0s - loss: 0.1117 - auc_6: 0.9754 - accuracy: 0.92 - ETA: 0s - loss: 0.1117 - auc_6: 0.9753 - accuracy: 0.91 - ETA: 0s - loss: 0.1118 - auc_6: 0.9753 - accuracy: 0.91 - ETA: 0s - loss: 0.1119 - auc_6: 0.9753 - accuracy: 0.91 - ETA: 0s - loss: 0.1119 - auc_6: 0.9753 - accuracy: 0.91 - ETA: 0s - loss: 0.1118 - auc_6: 0.9753 - accuracy: 0.91 - 3s 61us/sample - loss: 0.1119 - auc_6: 0.9752 - accuracy: 0.9198\n",
      "Epoch 4/10\n",
      "42436/42436 [==============================] - ETA: 6s - loss: 0.0769 - auc_6: 1.0000 - accuracy: 0.96 - ETA: 2s - loss: 0.1101 - auc_6: 0.9739 - accuracy: 0.92 - ETA: 2s - loss: 0.1093 - auc_6: 0.9763 - accuracy: 0.92 - ETA: 2s - loss: 0.1108 - auc_6: 0.9757 - accuracy: 0.92 - ETA: 2s - loss: 0.1128 - auc_6: 0.9737 - accuracy: 0.91 - ETA: 2s - loss: 0.1123 - auc_6: 0.9739 - accuracy: 0.91 - ETA: 2s - loss: 0.1120 - auc_6: 0.9742 - accuracy: 0.91 - ETA: 2s - loss: 0.1111 - auc_6: 0.9751 - accuracy: 0.92 - ETA: 2s - loss: 0.1114 - auc_6: 0.9750 - accuracy: 0.92 - ETA: 2s - loss: 0.1116 - auc_6: 0.9751 - accuracy: 0.92 - ETA: 2s - loss: 0.1115 - auc_6: 0.9750 - accuracy: 0.92 - ETA: 2s - loss: 0.1116 - auc_6: 0.9748 - accuracy: 0.92 - ETA: 2s - loss: 0.1111 - auc_6: 0.9749 - accuracy: 0.92 - ETA: 2s - loss: 0.1103 - auc_6: 0.9754 - accuracy: 0.92 - ETA: 1s - loss: 0.1106 - auc_6: 0.9753 - accuracy: 0.92 - ETA: 1s - loss: 0.1112 - auc_6: 0.9747 - accuracy: 0.92 - ETA: 1s - loss: 0.1111 - auc_6: 0.9749 - accuracy: 0.92 - ETA: 1s - loss: 0.1112 - auc_6: 0.9748 - accuracy: 0.92 - ETA: 1s - loss: 0.1108 - auc_6: 0.9751 - accuracy: 0.92 - ETA: 1s - loss: 0.1106 - auc_6: 0.9752 - accuracy: 0.92 - ETA: 1s - loss: 0.1104 - auc_6: 0.9752 - accuracy: 0.92 - ETA: 1s - loss: 0.1102 - auc_6: 0.9754 - accuracy: 0.92 - ETA: 1s - loss: 0.1097 - auc_6: 0.9757 - accuracy: 0.92 - ETA: 1s - loss: 0.1099 - auc_6: 0.9756 - accuracy: 0.92 - ETA: 1s - loss: 0.1100 - auc_6: 0.9757 - accuracy: 0.92 - ETA: 1s - loss: 0.1100 - auc_6: 0.9757 - accuracy: 0.92 - ETA: 1s - loss: 0.1099 - auc_6: 0.9759 - accuracy: 0.92 - ETA: 1s - loss: 0.1100 - auc_6: 0.9757 - accuracy: 0.92 - ETA: 1s - loss: 0.1101 - auc_6: 0.9756 - accuracy: 0.92 - ETA: 1s - loss: 0.1101 - auc_6: 0.9755 - accuracy: 0.92 - ETA: 1s - loss: 0.1100 - auc_6: 0.9756 - accuracy: 0.92 - ETA: 1s - loss: 0.1101 - auc_6: 0.9756 - accuracy: 0.92 - ETA: 0s - loss: 0.1101 - auc_6: 0.9756 - accuracy: 0.92 - ETA: 0s - loss: 0.1098 - auc_6: 0.9759 - accuracy: 0.92 - ETA: 0s - loss: 0.1098 - auc_6: 0.9759 - accuracy: 0.92 - ETA: 0s - loss: 0.1096 - auc_6: 0.9760 - accuracy: 0.92 - ETA: 0s - loss: 0.1096 - auc_6: 0.9761 - accuracy: 0.92 - ETA: 0s - loss: 0.1095 - auc_6: 0.9762 - accuracy: 0.92 - ETA: 0s - loss: 0.1094 - auc_6: 0.9762 - accuracy: 0.92 - ETA: 0s - loss: 0.1094 - auc_6: 0.9763 - accuracy: 0.92 - ETA: 0s - loss: 0.1092 - auc_6: 0.9765 - accuracy: 0.92 - ETA: 0s - loss: 0.1092 - auc_6: 0.9764 - accuracy: 0.92 - ETA: 0s - loss: 0.1090 - auc_6: 0.9765 - accuracy: 0.92 - ETA: 0s - loss: 0.1090 - auc_6: 0.9766 - accuracy: 0.92 - ETA: 0s - loss: 0.1089 - auc_6: 0.9766 - accuracy: 0.92 - ETA: 0s - loss: 0.1087 - auc_6: 0.9767 - accuracy: 0.92 - ETA: 0s - loss: 0.1086 - auc_6: 0.9768 - accuracy: 0.92 - ETA: 0s - loss: 0.1086 - auc_6: 0.9768 - accuracy: 0.92 - ETA: 0s - loss: 0.1087 - auc_6: 0.9766 - accuracy: 0.92 - ETA: 0s - loss: 0.1089 - auc_6: 0.9764 - accuracy: 0.92 - ETA: 0s - loss: 0.1089 - auc_6: 0.9764 - accuracy: 0.92 - ETA: 0s - loss: 0.1088 - auc_6: 0.9765 - accuracy: 0.92 - 3s 63us/sample - loss: 0.1086 - auc_6: 0.9767 - accuracy: 0.9225\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42436/42436 [==============================] - ETA: 6s - loss: 0.1109 - auc_6: 0.9784 - accuracy: 0.93 - ETA: 2s - loss: 0.1104 - auc_6: 0.9762 - accuracy: 0.91 - ETA: 2s - loss: 0.1048 - auc_6: 0.9803 - accuracy: 0.92 - ETA: 2s - loss: 0.1067 - auc_6: 0.9787 - accuracy: 0.92 - ETA: 2s - loss: 0.1088 - auc_6: 0.9768 - accuracy: 0.92 - ETA: 2s - loss: 0.1092 - auc_6: 0.9760 - accuracy: 0.92 - ETA: 2s - loss: 0.1086 - auc_6: 0.9758 - accuracy: 0.92 - ETA: 2s - loss: 0.1077 - auc_6: 0.9765 - accuracy: 0.92 - ETA: 2s - loss: 0.1085 - auc_6: 0.9761 - accuracy: 0.92 - ETA: 2s - loss: 0.1080 - auc_6: 0.9765 - accuracy: 0.92 - ETA: 2s - loss: 0.1071 - auc_6: 0.9774 - accuracy: 0.92 - ETA: 2s - loss: 0.1079 - auc_6: 0.9765 - accuracy: 0.92 - ETA: 2s - loss: 0.1080 - auc_6: 0.9765 - accuracy: 0.92 - ETA: 2s - loss: 0.1079 - auc_6: 0.9764 - accuracy: 0.92 - ETA: 2s - loss: 0.1086 - auc_6: 0.9758 - accuracy: 0.92 - ETA: 2s - loss: 0.1085 - auc_6: 0.9758 - accuracy: 0.92 - ETA: 2s - loss: 0.1090 - auc_6: 0.9752 - accuracy: 0.92 - ETA: 1s - loss: 0.1089 - auc_6: 0.9754 - accuracy: 0.92 - ETA: 1s - loss: 0.1085 - auc_6: 0.9757 - accuracy: 0.92 - ETA: 1s - loss: 0.1084 - auc_6: 0.9758 - accuracy: 0.92 - ETA: 1s - loss: 0.1084 - auc_6: 0.9759 - accuracy: 0.92 - ETA: 1s - loss: 0.1080 - auc_6: 0.9762 - accuracy: 0.92 - ETA: 1s - loss: 0.1077 - auc_6: 0.9764 - accuracy: 0.92 - ETA: 1s - loss: 0.1077 - auc_6: 0.9765 - accuracy: 0.92 - ETA: 1s - loss: 0.1077 - auc_6: 0.9763 - accuracy: 0.92 - ETA: 1s - loss: 0.1081 - auc_6: 0.9761 - accuracy: 0.92 - ETA: 1s - loss: 0.1079 - auc_6: 0.9763 - accuracy: 0.92 - ETA: 1s - loss: 0.1080 - auc_6: 0.9762 - accuracy: 0.92 - ETA: 1s - loss: 0.1078 - auc_6: 0.9764 - accuracy: 0.92 - ETA: 1s - loss: 0.1077 - auc_6: 0.9764 - accuracy: 0.92 - ETA: 1s - loss: 0.1076 - auc_6: 0.9764 - accuracy: 0.92 - ETA: 1s - loss: 0.1075 - auc_6: 0.9765 - accuracy: 0.92 - ETA: 1s - loss: 0.1074 - auc_6: 0.9767 - accuracy: 0.92 - ETA: 1s - loss: 0.1074 - auc_6: 0.9768 - accuracy: 0.92 - ETA: 1s - loss: 0.1073 - auc_6: 0.9768 - accuracy: 0.92 - ETA: 0s - loss: 0.1073 - auc_6: 0.9767 - accuracy: 0.92 - ETA: 0s - loss: 0.1073 - auc_6: 0.9767 - accuracy: 0.92 - ETA: 0s - loss: 0.1071 - auc_6: 0.9769 - accuracy: 0.92 - ETA: 0s - loss: 0.1070 - auc_6: 0.9770 - accuracy: 0.92 - ETA: 0s - loss: 0.1069 - auc_6: 0.9770 - accuracy: 0.92 - ETA: 0s - loss: 0.1068 - auc_6: 0.9770 - accuracy: 0.92 - ETA: 0s - loss: 0.1067 - auc_6: 0.9771 - accuracy: 0.92 - ETA: 0s - loss: 0.1066 - auc_6: 0.9771 - accuracy: 0.92 - ETA: 0s - loss: 0.1066 - auc_6: 0.9770 - accuracy: 0.92 - ETA: 0s - loss: 0.1066 - auc_6: 0.9771 - accuracy: 0.92 - ETA: 0s - loss: 0.1065 - auc_6: 0.9772 - accuracy: 0.92 - ETA: 0s - loss: 0.1066 - auc_6: 0.9771 - accuracy: 0.92 - ETA: 0s - loss: 0.1064 - auc_6: 0.9772 - accuracy: 0.92 - ETA: 0s - loss: 0.1063 - auc_6: 0.9774 - accuracy: 0.92 - ETA: 0s - loss: 0.1063 - auc_6: 0.9774 - accuracy: 0.92 - ETA: 0s - loss: 0.1064 - auc_6: 0.9774 - accuracy: 0.92 - ETA: 0s - loss: 0.1064 - auc_6: 0.9773 - accuracy: 0.92 - ETA: 0s - loss: 0.1065 - auc_6: 0.9772 - accuracy: 0.92 - 3s 64us/sample - loss: 0.1065 - auc_6: 0.9772 - accuracy: 0.9239\n",
      "Epoch 6/10\n",
      "42436/42436 [==============================] - ETA: 7s - loss: 0.0729 - auc_6: 1.0000 - accuracy: 0.93 - ETA: 2s - loss: 0.0986 - auc_6: 0.9855 - accuracy: 0.93 - ETA: 2s - loss: 0.1048 - auc_6: 0.9795 - accuracy: 0.92 - ETA: 2s - loss: 0.1076 - auc_6: 0.9763 - accuracy: 0.92 - ETA: 2s - loss: 0.1070 - auc_6: 0.9771 - accuracy: 0.92 - ETA: 2s - loss: 0.1064 - auc_6: 0.9779 - accuracy: 0.92 - ETA: 2s - loss: 0.1051 - auc_6: 0.9788 - accuracy: 0.92 - ETA: 2s - loss: 0.1055 - auc_6: 0.9777 - accuracy: 0.92 - ETA: 2s - loss: 0.1050 - auc_6: 0.9780 - accuracy: 0.92 - ETA: 2s - loss: 0.1062 - auc_6: 0.9771 - accuracy: 0.92 - ETA: 2s - loss: 0.1061 - auc_6: 0.9768 - accuracy: 0.92 - ETA: 2s - loss: 0.1073 - auc_6: 0.9760 - accuracy: 0.92 - ETA: 2s - loss: 0.1074 - auc_6: 0.9762 - accuracy: 0.92 - ETA: 1s - loss: 0.1067 - auc_6: 0.9767 - accuracy: 0.92 - ETA: 1s - loss: 0.1060 - auc_6: 0.9773 - accuracy: 0.92 - ETA: 1s - loss: 0.1060 - auc_6: 0.9773 - accuracy: 0.92 - ETA: 1s - loss: 0.1057 - auc_6: 0.9774 - accuracy: 0.92 - ETA: 1s - loss: 0.1056 - auc_6: 0.9776 - accuracy: 0.92 - ETA: 1s - loss: 0.1057 - auc_6: 0.9773 - accuracy: 0.92 - ETA: 1s - loss: 0.1056 - auc_6: 0.9773 - accuracy: 0.92 - ETA: 1s - loss: 0.1058 - auc_6: 0.9771 - accuracy: 0.92 - ETA: 1s - loss: 0.1057 - auc_6: 0.9773 - accuracy: 0.92 - ETA: 1s - loss: 0.1055 - auc_6: 0.9772 - accuracy: 0.92 - ETA: 1s - loss: 0.1054 - auc_6: 0.9773 - accuracy: 0.92 - ETA: 1s - loss: 0.1057 - auc_6: 0.9771 - accuracy: 0.92 - ETA: 1s - loss: 0.1055 - auc_6: 0.9773 - accuracy: 0.92 - ETA: 1s - loss: 0.1054 - auc_6: 0.9773 - accuracy: 0.92 - ETA: 1s - loss: 0.1053 - auc_6: 0.9774 - accuracy: 0.92 - ETA: 1s - loss: 0.1052 - auc_6: 0.9775 - accuracy: 0.92 - ETA: 1s - loss: 0.1054 - auc_6: 0.9774 - accuracy: 0.92 - ETA: 1s - loss: 0.1055 - auc_6: 0.9774 - accuracy: 0.92 - ETA: 1s - loss: 0.1055 - auc_6: 0.9775 - accuracy: 0.92 - ETA: 0s - loss: 0.1057 - auc_6: 0.9774 - accuracy: 0.92 - ETA: 0s - loss: 0.1057 - auc_6: 0.9773 - accuracy: 0.92 - ETA: 0s - loss: 0.1055 - auc_6: 0.9774 - accuracy: 0.92 - ETA: 0s - loss: 0.1052 - auc_6: 0.9777 - accuracy: 0.92 - ETA: 0s - loss: 0.1056 - auc_6: 0.9774 - accuracy: 0.92 - ETA: 0s - loss: 0.1054 - auc_6: 0.9776 - accuracy: 0.92 - ETA: 0s - loss: 0.1051 - auc_6: 0.9778 - accuracy: 0.92 - ETA: 0s - loss: 0.1050 - auc_6: 0.9779 - accuracy: 0.92 - ETA: 0s - loss: 0.1052 - auc_6: 0.9778 - accuracy: 0.92 - ETA: 0s - loss: 0.1050 - auc_6: 0.9779 - accuracy: 0.92 - ETA: 0s - loss: 0.1050 - auc_6: 0.9779 - accuracy: 0.92 - ETA: 0s - loss: 0.1049 - auc_6: 0.9780 - accuracy: 0.92 - ETA: 0s - loss: 0.1049 - auc_6: 0.9780 - accuracy: 0.92 - ETA: 0s - loss: 0.1047 - auc_6: 0.9782 - accuracy: 0.92 - ETA: 0s - loss: 0.1047 - auc_6: 0.9781 - accuracy: 0.92 - ETA: 0s - loss: 0.1049 - auc_6: 0.9780 - accuracy: 0.92 - ETA: 0s - loss: 0.1049 - auc_6: 0.9780 - accuracy: 0.92 - ETA: 0s - loss: 0.1048 - auc_6: 0.9781 - accuracy: 0.92 - ETA: 0s - loss: 0.1047 - auc_6: 0.9782 - accuracy: 0.92 - ETA: 0s - loss: 0.1048 - auc_6: 0.9781 - accuracy: 0.92 - 3s 62us/sample - loss: 0.1048 - auc_6: 0.9781 - accuracy: 0.9274\n",
      "Epoch 7/10\n",
      "42436/42436 [==============================] - ETA: 6s - loss: 0.0721 - auc_6: 1.0000 - accuracy: 0.96 - ETA: 2s - loss: 0.1033 - auc_6: 0.9815 - accuracy: 0.92 - ETA: 2s - loss: 0.1060 - auc_6: 0.9795 - accuracy: 0.92 - ETA: 2s - loss: 0.1065 - auc_6: 0.9785 - accuracy: 0.92 - ETA: 2s - loss: 0.1046 - auc_6: 0.9800 - accuracy: 0.92 - ETA: 2s - loss: 0.1058 - auc_6: 0.9790 - accuracy: 0.92 - ETA: 2s - loss: 0.1051 - auc_6: 0.9793 - accuracy: 0.92 - ETA: 2s - loss: 0.1051 - auc_6: 0.9790 - accuracy: 0.92 - ETA: 2s - loss: 0.1054 - auc_6: 0.9785 - accuracy: 0.92 - ETA: 2s - loss: 0.1048 - auc_6: 0.9788 - accuracy: 0.92 - ETA: 2s - loss: 0.1046 - auc_6: 0.9792 - accuracy: 0.92 - ETA: 2s - loss: 0.1048 - auc_6: 0.9790 - accuracy: 0.92 - ETA: 2s - loss: 0.1046 - auc_6: 0.9791 - accuracy: 0.92 - ETA: 2s - loss: 0.1040 - auc_6: 0.9795 - accuracy: 0.92 - ETA: 1s - loss: 0.1036 - auc_6: 0.9796 - accuracy: 0.92 - ETA: 1s - loss: 0.1040 - auc_6: 0.9792 - accuracy: 0.92 - ETA: 1s - loss: 0.1040 - auc_6: 0.9792 - accuracy: 0.92 - ETA: 1s - loss: 0.1039 - auc_6: 0.9792 - accuracy: 0.92 - ETA: 1s - loss: 0.1034 - auc_6: 0.9794 - accuracy: 0.92 - ETA: 1s - loss: 0.1033 - auc_6: 0.9794 - accuracy: 0.92 - ETA: 1s - loss: 0.1032 - auc_6: 0.9796 - accuracy: 0.92 - ETA: 1s - loss: 0.1033 - auc_6: 0.9795 - accuracy: 0.92 - ETA: 1s - loss: 0.1037 - auc_6: 0.9791 - accuracy: 0.92 - ETA: 1s - loss: 0.1036 - auc_6: 0.9791 - accuracy: 0.92 - ETA: 1s - loss: 0.1036 - auc_6: 0.9792 - accuracy: 0.92 - ETA: 1s - loss: 0.1035 - auc_6: 0.9792 - accuracy: 0.92 - ETA: 1s - loss: 0.1034 - auc_6: 0.9793 - accuracy: 0.92 - ETA: 1s - loss: 0.1036 - auc_6: 0.9792 - accuracy: 0.92 - ETA: 1s - loss: 0.1037 - auc_6: 0.9790 - accuracy: 0.92 - ETA: 1s - loss: 0.1037 - auc_6: 0.9789 - accuracy: 0.92 - ETA: 1s - loss: 0.1037 - auc_6: 0.9790 - accuracy: 0.92 - ETA: 1s - loss: 0.1035 - auc_6: 0.9791 - accuracy: 0.92 - ETA: 0s - loss: 0.1033 - auc_6: 0.9792 - accuracy: 0.92 - ETA: 0s - loss: 0.1035 - auc_6: 0.9790 - accuracy: 0.92 - ETA: 0s - loss: 0.1035 - auc_6: 0.9790 - accuracy: 0.92 - ETA: 0s - loss: 0.1035 - auc_6: 0.9790 - accuracy: 0.92 - ETA: 0s - loss: 0.1035 - auc_6: 0.9790 - accuracy: 0.92 - ETA: 0s - loss: 0.1033 - auc_6: 0.9792 - accuracy: 0.92 - ETA: 0s - loss: 0.1033 - auc_6: 0.9791 - accuracy: 0.92 - ETA: 0s - loss: 0.1034 - auc_6: 0.9791 - accuracy: 0.92 - ETA: 0s - loss: 0.1034 - auc_6: 0.9791 - accuracy: 0.92 - ETA: 0s - loss: 0.1034 - auc_6: 0.9790 - accuracy: 0.92 - ETA: 0s - loss: 0.1034 - auc_6: 0.9791 - accuracy: 0.92 - ETA: 0s - loss: 0.1035 - auc_6: 0.9790 - accuracy: 0.92 - ETA: 0s - loss: 0.1034 - auc_6: 0.9790 - accuracy: 0.92 - ETA: 0s - loss: 0.1033 - auc_6: 0.9791 - accuracy: 0.92 - ETA: 0s - loss: 0.1034 - auc_6: 0.9790 - accuracy: 0.92 - ETA: 0s - loss: 0.1033 - auc_6: 0.9791 - accuracy: 0.92 - ETA: 0s - loss: 0.1033 - auc_6: 0.9791 - accuracy: 0.92 - ETA: 0s - loss: 0.1034 - auc_6: 0.9789 - accuracy: 0.92 - ETA: 0s - loss: 0.1034 - auc_6: 0.9789 - accuracy: 0.92 - ETA: 0s - loss: 0.1034 - auc_6: 0.9789 - accuracy: 0.92 - ETA: 0s - loss: 0.1033 - auc_6: 0.9790 - accuracy: 0.92 - 3s 64us/sample - loss: 0.1033 - auc_6: 0.9791 - accuracy: 0.9284\n",
      "Epoch 8/10\n",
      "42436/42436 [==============================] - ETA: 6s - loss: 0.0705 - auc_6: 1.0000 - accuracy: 1.00 - ETA: 2s - loss: 0.0964 - auc_6: 0.9845 - accuracy: 0.94 - ETA: 2s - loss: 0.0972 - auc_6: 0.9842 - accuracy: 0.94 - ETA: 2s - loss: 0.0987 - auc_6: 0.9827 - accuracy: 0.93 - ETA: 2s - loss: 0.1004 - auc_6: 0.9811 - accuracy: 0.93 - ETA: 2s - loss: 0.1016 - auc_6: 0.9800 - accuracy: 0.93 - ETA: 2s - loss: 0.1015 - auc_6: 0.9802 - accuracy: 0.93 - ETA: 2s - loss: 0.1026 - auc_6: 0.9794 - accuracy: 0.93 - ETA: 2s - loss: 0.1022 - auc_6: 0.9799 - accuracy: 0.93 - ETA: 2s - loss: 0.1020 - auc_6: 0.9803 - accuracy: 0.93 - ETA: 2s - loss: 0.1026 - auc_6: 0.9797 - accuracy: 0.93 - ETA: 2s - loss: 0.1027 - auc_6: 0.9795 - accuracy: 0.93 - ETA: 2s - loss: 0.1024 - auc_6: 0.9797 - accuracy: 0.93 - ETA: 2s - loss: 0.1015 - auc_6: 0.9804 - accuracy: 0.93 - ETA: 2s - loss: 0.1011 - auc_6: 0.9809 - accuracy: 0.93 - ETA: 1s - loss: 0.1011 - auc_6: 0.9809 - accuracy: 0.93 - ETA: 1s - loss: 0.1015 - auc_6: 0.9805 - accuracy: 0.93 - ETA: 1s - loss: 0.1012 - auc_6: 0.9807 - accuracy: 0.93 - ETA: 1s - loss: 0.1014 - auc_6: 0.9804 - accuracy: 0.93 - ETA: 1s - loss: 0.1017 - auc_6: 0.9802 - accuracy: 0.93 - ETA: 1s - loss: 0.1021 - auc_6: 0.9800 - accuracy: 0.93 - ETA: 1s - loss: 0.1020 - auc_6: 0.9801 - accuracy: 0.93 - ETA: 1s - loss: 0.1020 - auc_6: 0.9800 - accuracy: 0.93 - ETA: 1s - loss: 0.1020 - auc_6: 0.9799 - accuracy: 0.93 - ETA: 1s - loss: 0.1021 - auc_6: 0.9798 - accuracy: 0.93 - ETA: 1s - loss: 0.1022 - auc_6: 0.9799 - accuracy: 0.93 - ETA: 1s - loss: 0.1020 - auc_6: 0.9800 - accuracy: 0.93 - ETA: 1s - loss: 0.1023 - auc_6: 0.9796 - accuracy: 0.93 - ETA: 1s - loss: 0.1024 - auc_6: 0.9796 - accuracy: 0.93 - ETA: 1s - loss: 0.1027 - auc_6: 0.9794 - accuracy: 0.93 - ETA: 1s - loss: 0.1029 - auc_6: 0.9794 - accuracy: 0.93 - ETA: 1s - loss: 0.1029 - auc_6: 0.9793 - accuracy: 0.92 - ETA: 1s - loss: 0.1029 - auc_6: 0.9793 - accuracy: 0.93 - ETA: 1s - loss: 0.1028 - auc_6: 0.9793 - accuracy: 0.93 - ETA: 0s - loss: 0.1027 - auc_6: 0.9794 - accuracy: 0.93 - ETA: 0s - loss: 0.1027 - auc_6: 0.9794 - accuracy: 0.93 - ETA: 0s - loss: 0.1026 - auc_6: 0.9795 - accuracy: 0.93 - ETA: 0s - loss: 0.1025 - auc_6: 0.9796 - accuracy: 0.93 - ETA: 0s - loss: 0.1023 - auc_6: 0.9798 - accuracy: 0.93 - ETA: 0s - loss: 0.1023 - auc_6: 0.9797 - accuracy: 0.93 - ETA: 0s - loss: 0.1023 - auc_6: 0.9798 - accuracy: 0.93 - ETA: 0s - loss: 0.1021 - auc_6: 0.9799 - accuracy: 0.93 - ETA: 0s - loss: 0.1021 - auc_6: 0.9799 - accuracy: 0.93 - ETA: 0s - loss: 0.1022 - auc_6: 0.9798 - accuracy: 0.93 - ETA: 0s - loss: 0.1023 - auc_6: 0.9797 - accuracy: 0.93 - ETA: 0s - loss: 0.1023 - auc_6: 0.9797 - accuracy: 0.93 - ETA: 0s - loss: 0.1023 - auc_6: 0.9797 - accuracy: 0.93 - ETA: 0s - loss: 0.1023 - auc_6: 0.9797 - accuracy: 0.93 - ETA: 0s - loss: 0.1025 - auc_6: 0.9795 - accuracy: 0.93 - ETA: 0s - loss: 0.1023 - auc_6: 0.9796 - accuracy: 0.93 - ETA: 0s - loss: 0.1023 - auc_6: 0.9797 - accuracy: 0.93 - ETA: 0s - loss: 0.1020 - auc_6: 0.9799 - accuracy: 0.93 - ETA: 0s - loss: 0.1021 - auc_6: 0.9798 - accuracy: 0.93 - ETA: 0s - loss: 0.1022 - auc_6: 0.9799 - accuracy: 0.93 - 3s 65us/sample - loss: 0.1021 - auc_6: 0.9799 - accuracy: 0.9308\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42436/42436 [==============================] - ETA: 9s - loss: 0.0999 - auc_6: 0.9843 - accuracy: 0.93 - ETA: 2s - loss: 0.1138 - auc_6: 0.9704 - accuracy: 0.91 - ETA: 2s - loss: 0.1073 - auc_6: 0.9754 - accuracy: 0.92 - ETA: 2s - loss: 0.1061 - auc_6: 0.9761 - accuracy: 0.92 - ETA: 2s - loss: 0.1057 - auc_6: 0.9762 - accuracy: 0.92 - ETA: 2s - loss: 0.1044 - auc_6: 0.9777 - accuracy: 0.92 - ETA: 2s - loss: 0.1036 - auc_6: 0.9781 - accuracy: 0.92 - ETA: 2s - loss: 0.1047 - auc_6: 0.9774 - accuracy: 0.92 - ETA: 2s - loss: 0.1044 - auc_6: 0.9776 - accuracy: 0.92 - ETA: 2s - loss: 0.1048 - auc_6: 0.9775 - accuracy: 0.92 - ETA: 2s - loss: 0.1040 - auc_6: 0.9783 - accuracy: 0.92 - ETA: 2s - loss: 0.1033 - auc_6: 0.9786 - accuracy: 0.93 - ETA: 2s - loss: 0.1026 - auc_6: 0.9793 - accuracy: 0.93 - ETA: 2s - loss: 0.1024 - auc_6: 0.9795 - accuracy: 0.93 - ETA: 2s - loss: 0.1017 - auc_6: 0.9802 - accuracy: 0.93 - ETA: 1s - loss: 0.1018 - auc_6: 0.9800 - accuracy: 0.93 - ETA: 1s - loss: 0.1018 - auc_6: 0.9799 - accuracy: 0.93 - ETA: 1s - loss: 0.1013 - auc_6: 0.9802 - accuracy: 0.93 - ETA: 1s - loss: 0.1011 - auc_6: 0.9805 - accuracy: 0.93 - ETA: 1s - loss: 0.1012 - auc_6: 0.9803 - accuracy: 0.93 - ETA: 1s - loss: 0.1011 - auc_6: 0.9804 - accuracy: 0.93 - ETA: 1s - loss: 0.1008 - auc_6: 0.9807 - accuracy: 0.93 - ETA: 1s - loss: 0.1008 - auc_6: 0.9806 - accuracy: 0.93 - ETA: 1s - loss: 0.1004 - auc_6: 0.9808 - accuracy: 0.93 - ETA: 1s - loss: 0.1000 - auc_6: 0.9810 - accuracy: 0.93 - ETA: 1s - loss: 0.1001 - auc_6: 0.9809 - accuracy: 0.93 - ETA: 1s - loss: 0.1004 - auc_6: 0.9807 - accuracy: 0.93 - ETA: 1s - loss: 0.1002 - auc_6: 0.9808 - accuracy: 0.93 - ETA: 1s - loss: 0.1002 - auc_6: 0.9808 - accuracy: 0.93 - ETA: 1s - loss: 0.1002 - auc_6: 0.9807 - accuracy: 0.93 - ETA: 1s - loss: 0.1002 - auc_6: 0.9807 - accuracy: 0.93 - ETA: 1s - loss: 0.1002 - auc_6: 0.9807 - accuracy: 0.93 - ETA: 1s - loss: 0.1004 - auc_6: 0.9806 - accuracy: 0.93 - ETA: 1s - loss: 0.1005 - auc_6: 0.9806 - accuracy: 0.93 - ETA: 1s - loss: 0.1004 - auc_6: 0.9807 - accuracy: 0.93 - ETA: 0s - loss: 0.1005 - auc_6: 0.9807 - accuracy: 0.93 - ETA: 0s - loss: 0.1005 - auc_6: 0.9807 - accuracy: 0.93 - ETA: 0s - loss: 0.1005 - auc_6: 0.9807 - accuracy: 0.93 - ETA: 0s - loss: 0.1005 - auc_6: 0.9807 - accuracy: 0.93 - ETA: 0s - loss: 0.1008 - auc_6: 0.9804 - accuracy: 0.93 - ETA: 0s - loss: 0.1009 - auc_6: 0.9803 - accuracy: 0.93 - ETA: 0s - loss: 0.1010 - auc_6: 0.9803 - accuracy: 0.93 - ETA: 0s - loss: 0.1009 - auc_6: 0.9803 - accuracy: 0.93 - ETA: 0s - loss: 0.1007 - auc_6: 0.9805 - accuracy: 0.93 - ETA: 0s - loss: 0.1007 - auc_6: 0.9805 - accuracy: 0.93 - ETA: 0s - loss: 0.1005 - auc_6: 0.9806 - accuracy: 0.93 - ETA: 0s - loss: 0.1007 - auc_6: 0.9805 - accuracy: 0.93 - ETA: 0s - loss: 0.1010 - auc_6: 0.9803 - accuracy: 0.93 - ETA: 0s - loss: 0.1010 - auc_6: 0.9803 - accuracy: 0.93 - ETA: 0s - loss: 0.1011 - auc_6: 0.9802 - accuracy: 0.93 - ETA: 0s - loss: 0.1010 - auc_6: 0.9803 - accuracy: 0.93 - ETA: 0s - loss: 0.1009 - auc_6: 0.9804 - accuracy: 0.93 - ETA: 0s - loss: 0.1009 - auc_6: 0.9804 - accuracy: 0.93 - ETA: 0s - loss: 0.1009 - auc_6: 0.9804 - accuracy: 0.93 - 3s 64us/sample - loss: 0.1009 - auc_6: 0.9804 - accuracy: 0.9327\n",
      "Epoch 10/10\n",
      "42436/42436 [==============================] - ETA: 6s - loss: 0.1253 - auc_6: 0.9471 - accuracy: 0.90 - ETA: 2s - loss: 0.1053 - auc_6: 0.9747 - accuracy: 0.91 - ETA: 2s - loss: 0.1061 - auc_6: 0.9760 - accuracy: 0.91 - ETA: 2s - loss: 0.1007 - auc_6: 0.9800 - accuracy: 0.92 - ETA: 2s - loss: 0.1005 - auc_6: 0.9799 - accuracy: 0.93 - ETA: 2s - loss: 0.1020 - auc_6: 0.9786 - accuracy: 0.93 - ETA: 2s - loss: 0.1012 - auc_6: 0.9798 - accuracy: 0.93 - ETA: 2s - loss: 0.1007 - auc_6: 0.9803 - accuracy: 0.93 - ETA: 2s - loss: 0.1002 - auc_6: 0.9809 - accuracy: 0.93 - ETA: 2s - loss: 0.1001 - auc_6: 0.9812 - accuracy: 0.93 - ETA: 2s - loss: 0.0994 - auc_6: 0.9818 - accuracy: 0.93 - ETA: 2s - loss: 0.0997 - auc_6: 0.9817 - accuracy: 0.93 - ETA: 1s - loss: 0.0999 - auc_6: 0.9815 - accuracy: 0.93 - ETA: 1s - loss: 0.0998 - auc_6: 0.9817 - accuracy: 0.93 - ETA: 1s - loss: 0.1003 - auc_6: 0.9814 - accuracy: 0.93 - ETA: 1s - loss: 0.1003 - auc_6: 0.9815 - accuracy: 0.93 - ETA: 1s - loss: 0.1005 - auc_6: 0.9814 - accuracy: 0.93 - ETA: 1s - loss: 0.1001 - auc_6: 0.9816 - accuracy: 0.93 - ETA: 1s - loss: 0.0999 - auc_6: 0.9817 - accuracy: 0.93 - ETA: 1s - loss: 0.1000 - auc_6: 0.9818 - accuracy: 0.93 - ETA: 1s - loss: 0.0998 - auc_6: 0.9819 - accuracy: 0.93 - ETA: 1s - loss: 0.0994 - auc_6: 0.9821 - accuracy: 0.93 - ETA: 1s - loss: 0.0993 - auc_6: 0.9821 - accuracy: 0.93 - ETA: 1s - loss: 0.0998 - auc_6: 0.9817 - accuracy: 0.93 - ETA: 1s - loss: 0.0999 - auc_6: 0.9817 - accuracy: 0.93 - ETA: 1s - loss: 0.0999 - auc_6: 0.9816 - accuracy: 0.93 - ETA: 1s - loss: 0.0996 - auc_6: 0.9818 - accuracy: 0.93 - ETA: 1s - loss: 0.0996 - auc_6: 0.9819 - accuracy: 0.93 - ETA: 1s - loss: 0.0996 - auc_6: 0.9818 - accuracy: 0.93 - ETA: 1s - loss: 0.0997 - auc_6: 0.9817 - accuracy: 0.93 - ETA: 0s - loss: 0.0998 - auc_6: 0.9816 - accuracy: 0.93 - ETA: 0s - loss: 0.1000 - auc_6: 0.9815 - accuracy: 0.93 - ETA: 0s - loss: 0.1000 - auc_6: 0.9814 - accuracy: 0.93 - ETA: 0s - loss: 0.0999 - auc_6: 0.9816 - accuracy: 0.93 - ETA: 0s - loss: 0.1001 - auc_6: 0.9814 - accuracy: 0.93 - ETA: 0s - loss: 0.1000 - auc_6: 0.9815 - accuracy: 0.93 - ETA: 0s - loss: 0.1002 - auc_6: 0.9814 - accuracy: 0.93 - ETA: 0s - loss: 0.1003 - auc_6: 0.9812 - accuracy: 0.93 - ETA: 0s - loss: 0.1003 - auc_6: 0.9813 - accuracy: 0.93 - ETA: 0s - loss: 0.1001 - auc_6: 0.9813 - accuracy: 0.93 - ETA: 0s - loss: 0.1000 - auc_6: 0.9814 - accuracy: 0.93 - ETA: 0s - loss: 0.0998 - auc_6: 0.9815 - accuracy: 0.93 - ETA: 0s - loss: 0.1000 - auc_6: 0.9813 - accuracy: 0.93 - ETA: 0s - loss: 0.0999 - auc_6: 0.9814 - accuracy: 0.93 - ETA: 0s - loss: 0.0998 - auc_6: 0.9815 - accuracy: 0.93 - ETA: 0s - loss: 0.0998 - auc_6: 0.9815 - accuracy: 0.93 - ETA: 0s - loss: 0.0999 - auc_6: 0.9813 - accuracy: 0.93 - ETA: 0s - loss: 0.0999 - auc_6: 0.9814 - accuracy: 0.93 - ETA: 0s - loss: 0.0998 - auc_6: 0.9814 - accuracy: 0.93 - ETA: 0s - loss: 0.0999 - auc_6: 0.9814 - accuracy: 0.93 - ETA: 0s - loss: 0.1000 - auc_6: 0.9813 - accuracy: 0.93 - 3s 61us/sample - loss: 0.1000 - auc_6: 0.9813 - accuracy: 0.9339\n",
      "dev auc: 0.9756731686713057\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "nn_clf =  keras.models.Sequential([\n",
    "            keras.layers.Dense(6, activation='relu', input_shape=(15,),kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            keras.layers.Dense(6, activation='relu',kernel_regularizer=keras.regularizers.l2(0.01) ),\n",
    "            keras.layers.Dense(6, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            keras.layers.Dense(6, activation='relu',kernel_regularizer=keras.regularizers.l2(0.01) ),\n",
    "            keras.layers.Dense(6, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            keras.layers.Dense(6, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "\n",
    "            keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "nn_clf.compile(loss='mse',metrics=[keras.metrics.AUC(), 'accuracy'])\n",
    "nn_clf.fit(np.array(trn_x),np.array(trn_y),epochs=10)\n",
    "dev_pred = nn_clf.predict(np.array(dev_x))\n",
    "print('dev auc:', roc_auc_score(dev_y, dev_pred[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_clf.predict_proba(test_x)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1452,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def write_file(predictions, file_path):\n",
    "    file = open(file_path, 'a', encoding='utf-8', newline='')\n",
    "    csv_writer = csv.writer(file, dialect='excel')\n",
    "    for p in predictions:\n",
    "        csv_writer.writerow([p, ])\n",
    "write_file(predictions,'test-prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
